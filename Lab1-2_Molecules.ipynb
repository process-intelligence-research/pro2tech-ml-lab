{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2 - Chemical case study**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple QSPR model using a feed forward neural network in PyTorch [[1](https://www.sciencedirect.com/science/article/pii/S0140700713003861)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our hands dirty on a chemical engineering example! In the following, we will predict the normal boiling point of pure refrigerants with a QSPR model and an artificial neural network (ANN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past decades, several models have been developed to predict properties of molecules by utilizing quantitative structure-property relationship ([QSPR](https://en.wikipedia.org/wiki/Quantitative_structure%E2%80%93activity_relationship)) modeling. In QSPR, the modeling process can be broken down into two steps: First, QSPR models introduce molecular descriptors $D = [d_1, d_2, ..., d_n]^T$ that depend on the structure of a molecule $m$. Second, a regression model $F(D):D → \\hat{p}$ is fitted that predicts a property $\\hat{p}$ as a function of $D$. The regression model is either linear or nonlinear, depending on the QSPR. QSPR models differ in the way they encode the molecular structure. Various descriptors have been used including structural group counts, the number of aromatic bonds, and topological indices, such as the [Wiener Index](https://en.wikipedia.org/wiki/Wiener_index) or branching indices. Development of QSPR models depends on the choice of informative descriptors, a selection process that requires domain knowledge and intuition. For the sake of time, we skip this process and use predefined descriptors. In our case study, the property $\\hat{p}$ is the normal boiling point and the regression model is of course an ANN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opposite to matlab, we do not have all functionalities pre installed in Python. Therefore, the first step for a project is always to install packages which extend our function library. Here, we install the following packages:\n",
    "- *torch*: The PyTorch package is the most important one for this lab and inherits a variety of useful functions for machine learning.\n",
    "- *scipy*: Package for scientific computing and technical computing with algorithms for optimization, interpolation, statistics, etc..\n",
    "- *matplotlib*: Provides plotting functions similar to matlab.\n",
    "- *tqdm*: Allows to print progress bar of for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install torch\n",
    "!pip install scipy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Python preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we import the previously installed packages into our script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional to the packages from the previous notebook we also need the $StandardScaler$ from scikit-learn for normalization of the dataset and the $DataLoader$ for additional training options. We also use *pandas* to handle the dataset we want to load from an external source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats # statistics tool for prob. dens. fun.\n",
    "from sklearn.preprocessing import StandardScaler # normalizes a dataset\n",
    "from sklearn.model_selection import train_test_split # randomly splits a dataset\n",
    "import pandas as pd # pandas package for data manipulation and analysis\n",
    "import numpy as np # numerical calculations in python\n",
    "import matplotlib.pyplot as plt # plotting similar to matlab\n",
    "import torch # PyTorch: the general machine learning framework in Python\n",
    "import torch.nn as nn # the artificial neural network module in PyTorch\n",
    "import torch.optim as optim  # contains optimizers for the backpropagation\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our results reproducable, we need to set a so-called \"seed\". Machine Learning includes stochastic processes in the weight/bias initialization and the backpropagation. By setting a seed in the program we make sure that always the same random numbers are chosen. Otherwise, we would get different results everytime we run this script, which is not nice for teaching purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line reads the dataset from an external .csv file into a Pandas dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = pd.read_table('boiling_point_data.csv', sep=',')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table contains several refrigerents with different descriptors and their normal boiling point obtained from experiments. The descriptors are the following:\n",
    "| Molecular Descriptor | Descriptor type | Descriptor definition |\n",
    "| --- | --- | --- |\n",
    "| R1e+ | GETAWAY descriptors | R maximal autocorrelation of lag 1/weighted by atomic Sanderson electronegativities |\n",
    "| MATS1m | 2D autocorrelation indices | Moran autocorrelation - lag 1/weighted by atomic masses |\n",
    "| X1sol | Connectivity indices | Solvation connectivity index chi-1 |\n",
    "| Me | Constitutional descriptors | Mean atomic Sanderson electronegativity (scaled on Carbon atom) |\n",
    "| ESpm02d | Edge adjacency indices | Spectral moment 02 from edge adj. matrix weighted by dipole moments |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We join all descriptors in one array for each molecule. Hence, we get a 2D array. This is our input array *x*. We show examplarily the array that describes the first molecule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Try out how many descriptors we actually need to make a reasonable prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data[['descriptor: R1e+', 'descriptor: MATS1m',\n",
    "                    'descriptor: X1sol', 'descriptor: Me',\n",
    "                    'descriptor: ESpm02d']].values.tolist())\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a problem if we just concatenate the different descriptors: They have quite different means and variances. This has a negative effect on the training of our ANN. For instance, a descriptor with very high values has a bigger effect than one with values close to 0 even though both might be equallly important. Hence, we need to standardize our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a function that can plot the normal distribution for the columns of a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norm(array):\n",
    "    for n in range(array[0].size): # plot the normal distribution for each descriptor\n",
    "        mu = np.mean(array[:,n]) # get the mean\n",
    "        std = np.std(array[:,n]) # get the standard deviation\n",
    "        input = np.linspace(mu - 3*std, mu + 3*std, 100) # range of 3 st. deviations\n",
    "        plt.plot(input, stats.norm.pdf(input, mu, std)) # plot normal dist of one descriptor\n",
    "    plt.title('Normal distribution')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the normal distributions of the raw descriptor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the scikit-learn *StandardScaling* to normalize our data. It normalizes the data to zero mean and a unit variance (standard deviation = 1). This improves the training performance of the neural network. If we plot the normal distribution again, we see that all descriptors share the same distribution now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = StandardScaler()\n",
    "x = st.fit_transform(x)\n",
    "plot_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target values for the regression *y* are simply the boiling points obtained from experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['experiment: Tboil /K'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to split the dataset in training, validation and test data. In the first step we will split the data in training and remaining dataset. Now since we want the valation and test size to be equal (10% each of overall data), we have to define *val_size=0.5* (that is 50% of remaining data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first split\n",
    "x_train, x_rem, y_train, y_rem = train_test_split(x, y, train_size=0.7)\n",
    "\n",
    "# second split\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_rem, y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the numpy arrays into PyTorch tensors. In this notebook we will introduce PyTorch datasets and dataloaders, therefore we do not need to unsqueeze the tensors or change the datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(x_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "x_val = torch.Tensor(x_val)\n",
    "y_val = torch.Tensor(y_val)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch works with *Dataset*s and *DataLoader*s (more information [here](https://pytorch.org/docs/stable/data.html)) to feed minibatches to the model during training. Using the DataLoader allows for advanced training methods. It’s easy to create a dataset from the already created tensors. We print out one example data point, containing the input and output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1431, -0.1922, -0.4408]), tensor(287.2500))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "val_set = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "test_set = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_loader will shuffle the data when an epoch has been used to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set,\n",
    "                            shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set,\n",
    "                            shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Set up neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the data we also nee to prepare our ANN. Pytorch uses a base model object and adds the layers and activations as other objects in a sequential manner. The first layer must get an input dimensions matching the data, whereas the following can deduce their input size from the previous layer. The output layer then must match the dimension of the target values. We start with a very simple model with a small hidden layer with ReLU activation. Very large networks will overfit, unless some form of regularization is put in place (such as early stopping or drop out). As the output values are continuous rather than class labels, the output dimension is a single neuron with a linear activation. Each ANN class needs a *forward* function which defines, how a signal propagates through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Here you can add one (or more) hidden layers to get a more complex neural network:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network definition\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.architecture = nn.Sequential(\n",
    "            # sequential model definition: add up layers & activation functions\n",
    "            nn.Linear(in_features=n_input, out_features=n_hidden), # hidden layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.Linear(in_features=n_hidden, out_features=n_output), # output layer\n",
    "        )\n",
    "    def forward(self, x): # feed forward path\n",
    "        out = self.architecture(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the  network can be trained the loss function and other hyperparameters need to be defined. The loss function is set for the usual mean squared error and the optimizer is stochastic gradient descent with a specific learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**This cell contains all relevant hyperparameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameters\n",
    "hidden_size = 10 # Number of neurons in the hidden layer\n",
    "learning_rate = 0.02  # The learning rate for the optimizer\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the hyperparameters to set up the ANN and the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(x[0]) # The input size should fit our fingerprint size\n",
    "output_size = 1  # The target is only the boiling point, so this will be one\n",
    "net = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the actual training of the neural network. We have two loss functions. The training loss and the validation loss. We use the validation loss to detect overfitting and make sure, that the model can generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Training pass\n",
    "    running_loss = 0\n",
    "    for input, target in train_loader:\n",
    "        optimizer.zero_grad() # Initialize the gradients, which will be recorded during the forward pass\n",
    "         \n",
    "        output = net(input) # Forward pass of the mini-batch\n",
    "        loss = criterion(output, target) # Computing the loss\n",
    "        loss.backward() # calculate the backward pass\n",
    "        optimizer.step() # Optimize the weights and bias\n",
    "        running_loss += loss.item()\n",
    "    train_loss.append(running_loss/len(train_loader))\n",
    "        \n",
    "    # Validation pass\n",
    "    running_val_loss = 0\n",
    "    for input_val, labels_val in val_loader:\n",
    "        output_val = net(input_val)\n",
    "        loss_val = criterion(output_val, labels_val)\n",
    "        running_val_loss += loss_val.item()\n",
    "    val_loss.append(running_val_loss/len(val_loader))\n",
    "\n",
    "    if epoch%10 == 0: # after 10 epochs we print the current status\n",
    "        print(\"Epoch: %3i\\t Training loss: %0.2F\\t Validation loss: \\\n",
    "                %0.2F\"%(epoch,(running_loss/len(train_loader)), \n",
    "                (running_val_loss/len(val_loader))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we plot the loss to see the training progress. The loss plot shows if adjustments need to be made to the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(train_loss, label='training')\n",
    "plt.plot(val_loss, label='validation', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss plot')\n",
    "plt.show()\n",
    "print('Final training loss: ')\n",
    "print(train_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Evaluate neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our model we compare the actual boiling point, which was measured in experiments, and the predicted boiling point from our neural network regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We switch to the evaluation mode of the model which turns of features like dropout. Then we get our predicted boiling points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred_train = net(x_train)\n",
    "    y_pred_val = net(x_val)\n",
    "    y_pred_test = net(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the root mean square error can be done using the pytorch operators. The .item() method converts single element tensors to Python scalers for printing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(( y_train - y_pred_train )**2).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(( y_val - y_pred_val )**2).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(( y_test - y_pred_test )**2).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the error on the held out test set is significantly higher than the validation set, this indicates overfitting.\n",
    "\n",
    "Finally, lets evaluate the model visually. The full blue line is the desired output which corresponds to an exact prediction. The dashed blue lines represent the 10% error threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, y_pred_train, alpha=0.1, label=\"Train\")\n",
    "plt.scatter(y_val, y_pred_val, alpha=0.3, label=\"Validation\")\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.xlabel('ground truth')\n",
    "plt.ylabel('prediction')\n",
    "min = np.min(data['experiment: Tboil /K'])\n",
    "max = np.max(data['experiment: Tboil /K'])\n",
    "plt.plot([min, max], [min, max], c=\"b\")\n",
    "plt.plot([min, max], [0.9*min, 0.9*max], c=\"b\", linestyle='--')\n",
    "plt.plot([min, max], [1.1*min, 1.1*max], c=\"b\", linestyle='--')\n",
    "plt.text(0.8*max, 0.65*max, '-10%')\n",
    "plt.text(0.6*max, 0.8*max, '+10%')\n",
    "plt.title('Pareto chart')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second half of the lab we demonstrated how to use a neural network for a complex regression like a QSPR model. For five given descriptors, we created a fingerprint and used it to train the neural network."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a6378035587bb97055001603ea9d85a2aa377cc6252a50ffca4355a71bc8b90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
